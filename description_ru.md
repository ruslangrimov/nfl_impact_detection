## Подготовка данных
Нарезал из видеороликов фрагменты размером **8х224х224х3**. Для позитивных примеров выбирал фреймы с импактами, брал **4 фрейма** перед ними и **3 после**. По сетке с заданным шагом нарезал фрагменты (импакт необязательно должен был быть по центру патча, так как у меня **one stage** решение было). Для негативных выбирал случайные фрагменты, на которых не содержалось импактов. Сохранял положение и размеры bbox'ов для фрагментов в отдельные файлы.

## Модели
С самого начала использовал 3D CNN, а конкретнее **I3D** из [SlowFast](https://github.com/facebookresearch/SlowFast). Единтсвенное, на вход добавил четвёртый канал, который для 5го фрейма был равен 1 (не знаю насколько это была полезная вещь), **именного для этого фрейма** сеть должна выдавать bbox'ы. Далее добавил в конце FPN с шестью каналами на выходе без upsampling из [pytorch_segmentation](https://github.com/qubvel/segmentation_models.pytorch).
**Брал 3d фичи со второго по пятый блок**, через `F.adaptive_avg_pool3d` приводил их к **2d виду**. Далее подавал их в FPN и получал **решётку размером 6x56x56**.
Эта решётка обучалась выдавать для пятого фрейма примерно тоже, что выдаёт yolo, но только без anchor'ов. Первый слой отвечал за наличие **центра bbox'а**. Четыре следующих - за положение **bbox'а внутри grid cell и его ширину и высоту**. Последний слой отвечал за **наличие impact'а**.

## Тренировка
### 1й этап. Тренировка на фрагментах:
Аугментации:
```python
A.Compose([
            A.HorizontalFlip(p=0.5),
            A.OneOf([
                A.ColorJitter(p=1),
                A.HueSaturationValue(40, 60, 40, p=1),
                A.RGBShift(60, 60, 60, p=1),
                A.RandomGamma((20, 200), p=1)
            ], p=0.8),
            A.OneOf([
                A.GaussianBlur(p=1),
                A.GaussNoise((10.0, 50.0), p=1),
                A.MedianBlur(blur_limit=5, p=1),
                A.MotionBlur(p=1),
            ], p=0.6),
            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0,
                            border_mode=0, p=0.4),
            # A.GridDistortion(p=0.2, border_mode=0),
        ])
```
#### Функция ошибки
Для позиции и размера bbox'а использовал **HuberLoss**, для наличия шлема и импакта **FocalBinaryLoss** или **`nn.BCEWithLogitsLoss`** с различными pos_weight в зависимости от конфига.

#### Расписание и оптимизатор
Дефолтный **Adam** и простое **уменьшение lr в 10** раз после определённого шага. Всего было 10 эпох тренировки.

#### Данные
Выбирал все положительные фрагменты и **количество_позитивных*2** негативных фрагментов. Так же начиная со второй эпохи использовал **hard negative mining**. Запускал инференс на всех негативных фагментах и выбирал те, где вероятность импакта была выше некоторого порога (0.1). Добавлял недостающие негативные экземпляры выбирая случайно негативные фрагменты.


### 2й этап тренировки
**Заморозил основную backbone** и пару эпох тренировал только FPN на полноразмерных картинках 8x720x1280x3. Аугментации и оптимизатор те же.

**Ансамбль из 4х таких моделей** выдавал примерно **0.48** на локальной валидации. Заметил что плохо работает детекция центров шлемов. Часто вместо шлемов сеть детектила наплечники и колени игроков как шлемы при том, что уверенность этих детектов была выше чем, например, некоторых шлемов.
Понял что это из-за того что сеть при тренировке в качестве бинарной маски получает только точки центров bbox'ов. При том что точка может быть не обязательно в центре шлема и сети трудно понять где выдавать активность а где - нет.
Поэтому вместо точек стал подавать круги с диаметром равным среднему ширины и высоты bbox'а. Круги постепенно размывались от центра до края. В качестве функции ошибки была всё таже `nn.BCEWithLogitsLoss` (да, так можно делать для задачи регрессии).
Натренировал отдельную **FPN сеть с resnet34 в качестве backbobe**, которой подавал на вход отдельные картинки без динамики и сегментацию с вышеописанными кругами.
Применение такой сети для детекта центров bbox'ов заметно повысило скор на локальный валидации примерно до **0.58**.

## Инференс
Всего было натренировано четыре сети на основе I3D с разными параметрами тренировки (одна без hard mining'а - при этом брались все имеющиеся негативные примеры). И две сети на основе FPN и resnet34 для сегментации шлемов. Предсказания сетей просто усреднялись. Включая предсказания параметров bbox'ов.

Для каждого фрейма видеозаписи начиная с пятого брал 4 предыдущих и три последующих фрейма. Подавал на вход I3D сетей данные размером **3x8x736x1280** и на выходе получал решётку **6x184x320**. На вход FPN-resnet34 подавал **3x736x1280** с текущим фреймом и получал сегментацию **1х736х1280**, которую дальше даунсамплил до размера 184x320. Дальше создавал список bbox'ов из скоров больше порога **0.4** из FPN-resnet34 и размеров из I3D и применял к ним **non maximum supression** с параметром **0.25**. Если вероятность импакта в центре шлема была больше **0.45**, то помечал такой шлем как положительный.

После того как получал импакты для каждого кадра в видео, применял подавления импактов для одного и того же шлема на соседних кадрах.

Тут было два варианта.

Первый - простой nms в диапазоне **+/-4** кадра с параметром **0.3**. Но в качестве максимума брал не вероятность шлема, а верояность импакта.
Второй - для импакта с самым большим скором находил bbox'ы с наибольшим iou выше некоторого порога на соседних фреймах. Подавлял их. Дальше уже брал эти, найденные, bbox'ы и находил на следующих фреймах наиболее похожие на них bbox'ы и т.д. **+/-4** фрейма.
Оба варианта давали примерно одинаковый скор.

## Постпроцессинг
Применял вид с двух сторон. Если на одном виде был импакт, а на другом виде не было ипакта в диапазоне **+/-1** фрейм, и скор этого импакта был меньше порога **0.55**, то такой импакт удалялся из сабмита.

Такой постпроцессинг докидывал немного, **0.01-0.015** к финальному скору.
